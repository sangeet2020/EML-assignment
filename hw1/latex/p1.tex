\documentclass{article}[a4paper]
\usepackage[a4paper, left=2.5cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{caption}
\captionsetup{width=.75\textwidth}
\usepackage[usestackEOL]{stackengine}
\usepackage{float}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,positioning,fit}
\usetikzlibrary{shapes,calc,arrows}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfiles}
\usepackage{blindtext}
\usepackage{hyperref}
\usepackage{float}
\usepackage{physics} % for 'pdv' macro
\usepackage{qtree}
\usepackage{stmaryrd}
\usepackage{multicol}
\usepackage{xparse}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{neuralnetwork}
\usepackage{pgfplots}
\usepackage{sidecap}
 
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

\pgfplotsset{compat=1.16}

% Syntax: \colorboxed[<color model>]{<color specification>}{<math formula>}
\newcommand*{\colorboxed}{}
\def\colorboxed#1#{%
  \colorboxedAux{#1}%
}
\newcommand*{\colorboxedAux}[3]{%
  % #1: optional argument for color model
  % #2: color specification
  % #3: formula
  \begingroup
    \colorlet{cb@saved}{.}%
    \color#1{#2}%
    \boxed{%
      \color{cb@saved}%
      #3%
    }%
  \endgroup
}

\def\XXX#1{\textcolor{red}{XXX #1}}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}
\newcommand\Mycomb[2][^n]{\prescript{#1\mkern-0.5mu}{}C_{#2}}
\def\XXX#1{\textcolor{red}{XXX #1}}
\newcommand{\vect}[1]{\boldsymbol{\textcolor{blue}{#1}}}

\title{\textbf{Elements of Machine Learning}\\
Assigment 1 - Problem 1
}
\author{ Sangeet Sagar(7009050), Philipp Schuhmacher(7010127)\\
        \texttt{\{sasa00001,phsc00003\}@stud.uni-saarland.de}
}
% \pgfplotsset{compat=1.17}
% \parindent 0in
% \parskip 0em
\begin{document}
\maketitle
\section*{1 Principles of statistical learning}
Explain in short and concise words the concepts of these pairs of terms:
\begin{enumerate}
    \item unsupervised and supervised learning
    \item prediction and inference
    \item classification and regression
    \item training and test data
    \item parametric and non-parametric models
\end{enumerate}
You are allowed to use a maximum of 300 words for this exercise, every ten more words will lead to losing one point.
\\~\\
\textbf{Solution:}\\
\begin{enumerate}
    \item In supervised learning, both input and output data points $(x_i, y_i)$ are supplied to build a model or learn a function that can be used to map new data points. Linear regression and logistic regression are commonly used supervised learning algorithms. \\
    However, in unsupervised learning, we are only given input data points $x_1, x_2, \hdots, x_n$, i.e. there is no information on the relationship between input and output variables, and the goal is to discover patterns or groupings in the data points. Clustering is a popular algorithm of this kind that is used to group similar data points.
    \item In prediction we use estimated function $\hat{f}$ learned on the training data, to predict output to unseen data points. In other words, output variable $Y$ is predicted using $\hat{Y} = \hat{f}(X)$.\\
Inference is about understanding the relationship between $X$ and $Y$ rather than estimating $f$, and the goal is to understand how the output is affected as the input variables change.
    
    \item  Classification is used to categorize a data point into a class or label. E.g. classifying an email as spam or safe. Regression uses training data to learn a function that best describes the relationship between input and output variables and then predict values for unknown data points.
    \item Data used to build (or train) a model or fit a function that holds the relationship between input and output data points is training data. Now the model built on train data must be evaluated to check it's performance on unseen data. This is done using test data. The test data provides an unbiased evaluation of the trained model.
    \item Parametric and non-parametric models are two different ways used to estimate $\hat{f}$ that best describes observations $(X, Y)$. The parametric model follows a 2-step approach. We assume a function $f$ and then use training data to estimate the coefficients of the function. For e.g. assume a linear function,
    \begin{align*}
        f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \hdots + \beta_pX_p
    \end{align*}
    the goal is to estimate these $p+1$ coefficients $\beta_0, \beta_1, \hdots, \beta_p$.
    While a non-parametric model does not make any assumption about the function $f$, it directly estimates $f$ that best fits the training data. This way, it can fit a wide range of possible shapes of $f$.
\end{enumerate}


\bibliographystyle{plainnat}
\bibliography{references}
\end{document}